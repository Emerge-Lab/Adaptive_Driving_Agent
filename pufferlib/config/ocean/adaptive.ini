[base]
package = ocean
env_name = adaptive_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 1
; backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[policy.conditioning]
type = "none"
collision_weight_lb = -1
collision_weight_ub = 0
offroad_weight_lb = -0.4
offroad_weight_ub = 0
goal_weight_lb = 0
goal_weight_ub = 1.0
entropy_weight_lb = 0.0
entropy_weight_ub = 0.003
discount_weight_lb = 0.8
discount_weight_ub = 0.98

[co_player_policy]
enabled = True # enabling this makes the training population play
num_ego_agents = 512
policy_name = Drive
rnn_name = Recurrent
policy_path = "pufferlib/resources/drive/policies/puffer_drive_tqq99nv9.pt"
input_size = 64
hidden_size = 256

[co_player_rnn]
input_size = 256
hidden_size = 256

[co_player_policy.conditioning]
type = "all"
collision_weight_lb = -1.0
collision_weight_ub = 0.0
offroad_weight_lb = 0.0
offroad_weight_ub = -0.2
goal_weight_lb = 0.0
goal_weight_ub = 1.0
entropy_weight_lb = 0.0
entropy_weight_ub = 0.001
discount_weight_lb = 0.98
discount_weight_ub = 0.98

[env]
num_agents = 1024
; Options: discrete, continuous
action_type = discrete
; Options: classic, jerk
dynamics_model = jerk
k_scenarios = 2
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.2
reward_ade = 0.0
dt = 0.1
reward_goal = 1.0
reward_goal_post_respawn = 0.25
; Meters around goal to be considered "reached"
goal_radius = 2.0
; What to do when the goal is reached. Options: 0:"respawn", 1:"generate_new_goals", 2:"stop"
goal_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
collision_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
offroad_behavior = 0
; Number of steps before reset
scenario_length = 91
resample_frequency = 910
num_maps = 1000
; Determines which step of the trajectory to initialize the agents at upon reset
init_steps = 0
; Options: "control_vehicles", "control_agents", "control_tracks_to_predict"
control_mode = "control_vehicles"
; Options: "created_all_valid", "create_only_controlled"
init_mode = "create_all_valid"


[train]
total_timesteps = 2_000_000_000
#learning_rate = 0.02
#gamma = 0.985
minibatch_multiplier = 32
anneal_lr = True
minibatch_size = 93184
max_minibatch_size = 372736
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.001
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.001
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 200
# Rendering options
render = True
render_interval = 200
obs_only = True # If True, show exactly what the agent sees in agent observation
show_grid = False # Show grid lines
show_lasers = False # Draws lines from ego agent observed ORUs and road elements to show detection range
show_human_logs = True # Display human xy logs in the background
render_map = none  # Options: str to path (e.g., "resources/drive/binaries/map_001.bin"), None
